{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepatation\n",
    "\n",
    "\n",
    "### NOTICE!!\n",
    "\n",
    "Out of date code, use data_prep.py for more up to date preprocessing of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import pyarrow\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sheet_names(sheets):\n",
    "    '''\n",
    "    Get rid of the duplicate sheets, only take the sheets from the \n",
    "    latest point in the day\n",
    "    '''\n",
    "    new_ranges = []\n",
    "    indices = []\n",
    "    \n",
    "    #Get all the tabs in the sheet \n",
    "    for s in sheets:\n",
    "        new_ranges.append(s.get(\"properties\", {}).get(\"title\"))\n",
    "    \n",
    "    # Remove all sheets that dont have a numeric header\n",
    "    new_ranges = [x for x in new_ranges if re.search(r'\\d', x)]\n",
    "    \n",
    "    #split the names to just get the date\n",
    "    clean_new_ranges = new_ranges.copy()\n",
    "    for i, x in enumerate(clean_new_ranges):\n",
    "        clean_new_ranges[i] = x.split('_')[0]   \n",
    "            \n",
    "    #Get the index of the latest tab for each date\n",
    "    for item in set(clean_new_ranges):\n",
    "        indices.append(clean_new_ranges.index(item))\n",
    "\n",
    "    clean_new_ranges = []\n",
    "    # Return wanted tabs for the sheet extraction\n",
    "    for index in sorted(indices):\n",
    "        clean_new_ranges.append(new_ranges[index])\n",
    "\n",
    "    return clean_new_ranges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modifying these scopes, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "SAMPLE_SPREADSHEET_ID = '1yZv9w9zRKwrGTaR-YzmAqMefw4wMlaXocejdxZaTs6w'\n",
    "\n",
    "\"\"\"Shows basic usage of the Sheets API.\n",
    "Prints values from a sample spreadsheet.\n",
    "\"\"\"\n",
    "\n",
    "creds = None\n",
    "# The file token.pickle stores the user's access and refresh tokens, and is\n",
    "# created automatically when the authorization flow completes for the first\n",
    "# time.\n",
    "if os.path.exists('token.pickle'):\n",
    "    with open('token.pickle', 'rb') as token:\n",
    "        creds = pickle.load(token)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\n",
    "            '../src/credentials.json', SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open('token.pickle', 'wb') as token:\n",
    "        pickle.dump(creds, token)\n",
    "        \n",
    "#get all the sheet names for ranges when querying\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "sheet_metadata = service.spreadsheets().get(spreadsheetId=SAMPLE_SPREADSHEET_ID).execute()\n",
    "sheets = sheet_metadata.get('sheets', '')\n",
    "\n",
    "# Clean the result to the sheet tabs we want\n",
    "cleaned_ranges = clean_sheet_names(sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Feb02_745pm',\n",
       " 'Feb01_11pm',\n",
       " 'Jan31_7pm',\n",
       " 'Jan30_930pm',\n",
       " 'Jan29_9pm',\n",
       " 'Jan28_11pm',\n",
       " 'Jan27_830pm',\n",
       " 'Jan26_11pm',\n",
       " 'Jan25_10pm',\n",
       " 'Jan24_12pm',\n",
       " 'Jan23_12pm',\n",
       " 'Jan22_12pm']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sheets to preprocess\n",
      "... Feb02_745pm\n",
      "... Feb01_11pm\n",
      "... Jan31_7pm\n",
      "... Jan30_930pm\n",
      "... Jan29_9pm\n",
      "... Jan28_11pm\n",
      "... Jan27_830pm\n",
      "... Jan26_11pm\n",
      "... Jan25_10pm\n",
      "... Jan24_12pm\n",
      "... Jan23_12pm\n",
      "... Jan22_12pm\n"
     ]
    }
   ],
   "source": [
    "# from time import strftime\n",
    "\n",
    "def fix_dates(tmp_df, tmp_sheet_range):\n",
    "\n",
    "    try:\n",
    "        # Get correct year\n",
    "        year = datetime.strptime(tmp_df['Last Update'][0].split(' ')[0], '%m-%d-%Y').year()\n",
    "        print(year)\n",
    "    except:\n",
    "        # Default to 2020\n",
    "        year = '2020'\n",
    "    \n",
    "    tmp_sheet_range = tmp_sheet_range.split('_')[0]\n",
    "    correct_date = datetime.strptime(tmp_sheet_range, '%b%d').strftime('%d/%m/' + year)    \n",
    "    tmp_df['Last Update'] = correct_date\n",
    "    \n",
    "    return tmp_df\n",
    "\n",
    "def get_data(sheet_range):\n",
    "    tmp_df = pd.DataFrame([])\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "                                range=sheet_range).execute()\n",
    "\n",
    "    header = result.get('values', [])[0]   # Assumes first line is header!\n",
    "    values = result.get('values', [])[1:]  # Everything else is data.\n",
    "    \n",
    "    \n",
    "    # rows with no deaths and recovered vals have shorter lists\n",
    "    # impute missing values with zeros\n",
    "    for i, row in enumerate(values):\n",
    "        if len(row) < len(header):\n",
    "            extra_zeros = (len(header) - len(row))\n",
    "            values[i] += [0] * extra_zeros\n",
    "\n",
    "    # Create Dataframe\n",
    "    if not values:\n",
    "        print('No data found.')\n",
    "    else:\n",
    "        all_data = []\n",
    "        for col_id, col_name in enumerate(header):\n",
    "            column_data = []\n",
    "            for row in values:\n",
    "                column_data.append(row[col_id])\n",
    "            ds = pd.Series(data=column_data, name=col_name)\n",
    "            all_data.append(ds)\n",
    "        tmp = pd.concat(all_data, axis=1)\n",
    "\n",
    "        tmp = fix_dates(tmp, sheet_range)\n",
    "        \n",
    "    print('...', sheet_range)\n",
    "    return tmp\n",
    "   \n",
    "df_list = []\n",
    "print('Getting sheets to preprocess')\n",
    "for sheet_range in cleaned_ranges:\n",
    "    df_list.append(get_data(sheet_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data we now need to clean it \n",
    "- Fill null values\n",
    "- remore suspected values\n",
    "- change column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning dataframes...\n"
     ]
    }
   ],
   "source": [
    "def clean_data(tmp_df):\n",
    "    if 'Demised' in tmp_df.columns:\n",
    "        tmp_df.rename(columns={'Demised':'Deaths'}, inplace=True)\n",
    "\n",
    "    if 'Country/Region' in tmp_df.columns:\n",
    "        tmp_df.rename(columns={'Country/Region':'country'}, inplace=True)\n",
    "    \n",
    "    if 'Province/State' in tmp_df.columns:\n",
    "        tmp_df.rename(columns={'Province/State':'province'}, inplace=True)\n",
    "      \n",
    "    if 'Last Update' in tmp_df.columns:\n",
    "        tmp_df.rename(columns={'Last Update':'date'}, inplace=True)\n",
    "        \n",
    "    if 'Suspected' in tmp_df.columns:\n",
    "        tmp_df = tmp_df.drop(columns='Suspected')\n",
    "\n",
    "    for col in tmp_df.columns:\n",
    "        tmp_df[col] = tmp_df[col].fillna(0)\n",
    "    \n",
    "    #Lower case all col names\n",
    "    tmp_df.columns = map(str.lower, tmp_df.columns)    \n",
    "    \n",
    "    return tmp_df\n",
    "\n",
    "cleaned_dataframes = []\n",
    "\n",
    "print('Cleaning dataframes...')\n",
    "for frame in df_list:\n",
    "    cleaned_dataframes.append(clean_data(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing columns...\n",
      "Concatenating all sheet dataframes into one...\n"
     ]
    }
   ],
   "source": [
    "#Impute the missing columns in the early stages with 0 values (recovered and deaths)\n",
    "print('Imputing missing columns...')\n",
    "cleaned_dataframes[-1]['recovered'] = [0] * (cleaned_dataframes[-1]).shape[0]\n",
    "cleaned_dataframes[-1]['deaths'] = [0] * (cleaned_dataframes[-1]).shape[0]\n",
    "\n",
    "cleaned_dataframes[-2]['recovered'] = [0] * (cleaned_dataframes[-2]).shape[0]\n",
    "cleaned_dataframes[-2]['deaths'] = [0] * (cleaned_dataframes[-2]).shape[0]\n",
    "\n",
    "print('Concatenating all sheet dataframes into one...')\n",
    "final_df = pd.concat(cleaned_dataframes, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confirmed</th>\n",
       "      <th>country</th>\n",
       "      <th>date</th>\n",
       "      <th>deaths</th>\n",
       "      <th>province</th>\n",
       "      <th>recovered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11177</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>02/02/2020</td>\n",
       "      <td>350</td>\n",
       "      <td>Hubei</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>661</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>02/02/2020</td>\n",
       "      <td>0</td>\n",
       "      <td>Zhejiang</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>02/02/2020</td>\n",
       "      <td>0</td>\n",
       "      <td>Guangdong</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>493</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>02/02/2020</td>\n",
       "      <td>2</td>\n",
       "      <td>Henan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>463</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>02/02/2020</td>\n",
       "      <td>0</td>\n",
       "      <td>Hunan</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  confirmed         country        date deaths   province recovered\n",
       "0     11177  Mainland China  02/02/2020    350      Hubei       295\n",
       "1       661  Mainland China  02/02/2020      0   Zhejiang        32\n",
       "2       632  Mainland China  02/02/2020      0  Guangdong        15\n",
       "3       493  Mainland China  02/02/2020      2      Henan        10\n",
       "4       463  Mainland China  02/02/2020      0      Hunan        16"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['date'] = final_df['date'].astype(str)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "01/02/2020    67\n",
       "02/02/2020    67\n",
       "31/01/2020    63\n",
       "30/01/2020    59\n",
       "29/01/2020    56\n",
       "27/01/2020    52\n",
       "28/01/2020    52\n",
       "26/01/2020    47\n",
       "23/01/2020    46\n",
       "25/01/2020    44\n",
       "24/01/2020    41\n",
       "22/01/2020    38\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # s = '1/22/2020 12:00 PM'\n",
    "# # dt = datetime.strptime(s,'%m/%d/%Y %H:%M PM')\n",
    "# # dt.date()\n",
    "# # dd = dt.date()\n",
    "# # print(dd)\n",
    "\n",
    "# def conver_date_time(date):\n",
    "    \n",
    "#     try:\n",
    "#         return datetime.strptime(date,'%m/%d/%Y %H:%M PM').date()\n",
    "#     except:\n",
    "#         return datetime.strptime(date, '%m/%d/%y %H:%M PM').date()\n",
    "\n",
    "\n",
    "# final_df['date'] = final_df['date'].apply(conver_date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "01/02/2020    9074661604493463340333262236231225183177159116...\n",
       "02/02/2020    1117766163249346334033330023623123019319115912...\n",
       "22/01/2020             1146102621415444401221100291514010110221\n",
       "23/01/2020        922952325351252444957132103616181401222713112\n",
       "24/01/2020    5495343362724232018151515109988554433322222211...\n",
       "25/01/2020    1052104988375696051444039363331191918151513131...\n",
       "26/01/2020    1423146128128110100706968635348474635232222211...\n",
       "27/01/2020    2714173168151132106100908780727066595135333330...\n",
       "28/01/2020    3554296241221206152147121109108999182805856484...\n",
       "29/01/2020    4586428311278277200165162145142129111101101786...\n",
       "30/01/2020    5806537393352332240237206178177168128121101878...\n",
       "31/01/2020    7153537436352332240238237184177168153139120968...\n",
       "Name: confirmed, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['confirmed'] = final_df['confirmed']\n",
    "final_df.groupby(['date']).confirmed.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting by date...\n"
     ]
    }
   ],
   "source": [
    "# sheets need to be sorted by date value\n",
    "print('Sorting by date...')\n",
    "final_df = final_df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "print('Saving...')\n",
    "file_name = './data/updated_{}.parquet.gzip'.format(datetime.date(datetime.now()))\n",
    "\n",
    "final_df.astype(str).to_parquet(file_name, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python36864bitanaconda3virtualenv59e2ff4492e04649af7e0fd703909eac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
