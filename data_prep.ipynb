{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepatation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import pyarrow\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sheet_names(sheets):\n",
    "    '''\n",
    "    Get rid of the duplicate sheets, only take the sheets from the \n",
    "    latest point in the day\n",
    "    '''\n",
    "    new_ranges = []\n",
    "    indices = []\n",
    "    \n",
    "    #Get all the tabs in the sheet \n",
    "    for s in sheets:\n",
    "        new_ranges.append(s.get(\"properties\", {}).get(\"title\"))\n",
    "        \n",
    "    #split the names to just get the date\n",
    "    clean_new_ranges = new_ranges.copy()\n",
    "    for i, x in enumerate(clean_new_ranges):\n",
    "        clean_new_ranges[i] = x.split('_')[0]    \n",
    "    \n",
    "    #Get the index of the latest tab for each date\n",
    "    for item in set(clean_new_ranges):\n",
    "        indices.append(clean_new_ranges.index(item))\n",
    "\n",
    "    clean_new_ranges = []\n",
    "    # Return wanted tabs for the sheet extraction\n",
    "    for index in sorted(indices):\n",
    "        clean_new_ranges.append(new_ranges[index])\n",
    "\n",
    "    return clean_new_ranges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modifying these scopes, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "SAMPLE_SPREADSHEET_ID = '1yZv9w9zRKwrGTaR-YzmAqMefw4wMlaXocejdxZaTs6w'\n",
    "\n",
    "\"\"\"Shows basic usage of the Sheets API.\n",
    "Prints values from a sample spreadsheet.\n",
    "\"\"\"\n",
    "\n",
    "creds = None\n",
    "# The file token.pickle stores the user's access and refresh tokens, and is\n",
    "# created automatically when the authorization flow completes for the first\n",
    "# time.\n",
    "if os.path.exists('token.pickle'):\n",
    "    with open('token.pickle', 'rb') as token:\n",
    "        creds = pickle.load(token)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\n",
    "            'credentials.json', SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open('token.pickle', 'wb') as token:\n",
    "        pickle.dump(creds, token)\n",
    "        \n",
    "#get all the sheet names for ranges when querying\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "sheet_metadata = service.spreadsheets().get(spreadsheetId=SAMPLE_SPREADSHEET_ID).execute()\n",
    "sheets = sheet_metadata.get('sheets', '')\n",
    "\n",
    "# Clean the result to the sheet tabs we want\n",
    "cleaned_ranges = clean_sheet_names(sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jan29_230pm',\n",
       " 'Jan28_11pm',\n",
       " 'Jan27_830pm',\n",
       " 'Jan26_11pm',\n",
       " 'Jan25_10pm',\n",
       " 'Jan24_12pm',\n",
       " 'Jan23_12pm',\n",
       " 'Jan22_12pm']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sheets to preprocess\n",
      "... Jan29_230pm\n",
      "... Jan28_11pm\n",
      "... Jan27_830pm\n",
      "... Jan26_11pm\n",
      "... Jan25_10pm\n",
      "... Jan24_12pm\n",
      "... Jan23_12pm\n",
      "... Jan22_12pm\n"
     ]
    }
   ],
   "source": [
    "def get_data(sheet_range):\n",
    "    tmp_df = pd.DataFrame([])\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "                                range=sheet_range).execute()\n",
    "\n",
    "    header = result.get('values', [])[0]   # Assumes first line is header!\n",
    "    values = result.get('values', [])[1:]  # Everything else is data.\n",
    "    \n",
    "    \n",
    "    # rows with no deaths and recovered vals have shorter lists\n",
    "    # impute missing values with zeros\n",
    "    for i, row in enumerate(values):\n",
    "        if len(row) < len(header):\n",
    "            extra_zeros = (len(header) - len(row))\n",
    "            values[i] += [0] * extra_zeros\n",
    "\n",
    "    # Create Dataframe\n",
    "    if not values:\n",
    "        print('No data found.')\n",
    "    else:\n",
    "        all_data = []\n",
    "        for col_id, col_name in enumerate(header):\n",
    "            column_data = []\n",
    "            for row in values:\n",
    "                column_data.append(row[col_id])\n",
    "            ds = pd.Series(data=column_data, name=col_name)\n",
    "            all_data.append(ds)\n",
    "        tmp = pd.concat(all_data, axis=1)\n",
    "        \n",
    "    print('...', sheet_range)\n",
    "    return tmp\n",
    "   \n",
    "df_list = []\n",
    "print('Getting sheets to preprocess')\n",
    "for sheet_range in cleaned_ranges:\n",
    "    df_list.append(get_data(sheet_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data we now need to clean it \n",
    "- Fill null values\n",
    "- remore suspected values\n",
    "- change column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning dataframes...\n"
     ]
    }
   ],
   "source": [
    "def clean_data(tmp_df):\n",
    "    if 'Demised' in tmp_df.columns:\n",
    "        tmp_df.rename(columns={'Demised':'Deaths'}, inplace=True)\n",
    "\n",
    "    if 'Country/Region' in tmp_df.columns:\n",
    "        tmp_df.rename(columns={'Country/Region':'country'}, inplace=True)\n",
    "    \n",
    "    if 'Province/State' in tmp_df.columns:\n",
    "        tmp_df.rename(columns={'Province/State':'province'}, inplace=True)\n",
    "      \n",
    "    if 'Last Update' in tmp_df.columns:\n",
    "        tmp_df.rename(columns={'Last Update':'date'}, inplace=True)\n",
    "        \n",
    "    if 'Suspected' in tmp_df.columns:\n",
    "        tmp_df = tmp_df.drop(columns='Suspected')\n",
    "\n",
    "    for col in tmp_df.columns:\n",
    "        tmp_df[col] = tmp_df[col].fillna(0)\n",
    "    \n",
    "    #Lower case all col names\n",
    "    tmp_df.columns = map(str.lower, tmp_df.columns)    \n",
    "    \n",
    "    return tmp_df\n",
    "\n",
    "cleaned_dataframes = []\n",
    "\n",
    "print('Cleaning dataframes...')\n",
    "for frame in df_list:\n",
    "    cleaned_dataframes.append(clean_data(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing columns...\n",
      "Concatenating all sheet dataframes into one...\n"
     ]
    }
   ],
   "source": [
    "#Impute the missing columns in the early stages with 0 values (recovered and deaths)\n",
    "print('Imputing missing columns...')\n",
    "cleaned_dataframes[-1]['recovered'] = [0] * (cleaned_dataframes[-1]).shape[0]\n",
    "cleaned_dataframes[-1]['deaths'] = [0] * (cleaned_dataframes[-1]).shape[0]\n",
    "\n",
    "cleaned_dataframes[-2]['recovered'] = [0] * (cleaned_dataframes[-2]).shape[0]\n",
    "cleaned_dataframes[-2]['deaths'] = [0] * (cleaned_dataframes[-2]).shape[0]\n",
    "\n",
    "print('Concatenating all sheet dataframes into one...')\n",
    "final_df = pd.concat(cleaned_dataframes, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confirmed</th>\n",
       "      <th>country</th>\n",
       "      <th>date</th>\n",
       "      <th>deaths</th>\n",
       "      <th>province</th>\n",
       "      <th>recovered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3554</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1/29/2020 14:30</td>\n",
       "      <td>125</td>\n",
       "      <td>Hubei</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>296</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1/29/2020 14:30</td>\n",
       "      <td></td>\n",
       "      <td>Zhejiang</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>277</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1/29/2020 14:30</td>\n",
       "      <td></td>\n",
       "      <td>Guangdong</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1/29/2020 14:30</td>\n",
       "      <td>0</td>\n",
       "      <td>Hunan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>206</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1/29/2020 14:30</td>\n",
       "      <td>2</td>\n",
       "      <td>Henan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  confirmed         country             date deaths   province recovered\n",
       "0      3554  Mainland China  1/29/2020 14:30    125      Hubei        88\n",
       "1       296  Mainland China  1/29/2020 14:30          Zhejiang         3\n",
       "2       277  Mainland China  1/29/2020 14:30         Guangdong         5\n",
       "3       221  Mainland China  1/29/2020 14:30      0      Hunan         0\n",
       "4       206  Mainland China  1/29/2020 14:30      2      Henan         1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['date'] = final_df['date'].astype(str)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1/29/2020 14:30       54\n",
       "1/27/2020 20:30       52\n",
       "1/28/2020 23:00       51\n",
       "1/26/2020 23:00       47\n",
       "1/23/20 12:00 PM      46\n",
       "1/25/2020 12:00 PM    42\n",
       "1/24/2020 12:00 PM    40\n",
       "1/22/2020 12:00       38\n",
       "1/25/2020 10:00 PM     2\n",
       "1/28/2020 18:00        1\n",
       "1/24/2020 4:00 PM      1\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = '1/22/2020 12:00 PM'\n",
    "# dt = datetime.strptime(s,'%m/%d/%Y %H:%M PM')\n",
    "# dt.date()\n",
    "# dd = dt.date()\n",
    "# print(dd)\n",
    "\n",
    "def conver_date_time(date):\n",
    "    \n",
    "    try:\n",
    "        if 'PM' in date:\n",
    "            return datetime.strptime(date,'%m/%d/%Y %H:%M PM').date()\n",
    "        elif 'AM' in date:\n",
    "            return datetime.strptime(date,'%m/%d/%Y %H:%M AM').date()\n",
    "        else:\n",
    "            return datetime.strptime(date,'%m/%d/%Y %H:%M').date()\n",
    "    except:\n",
    "        return datetime.strptime(date, '%m/%d/%y %H:%M PM').date()\n",
    "\n",
    "\n",
    "final_df['date'] = final_df['date'].apply(conver_date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.groupby(['date']).confirmed.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting by date...\n"
     ]
    }
   ],
   "source": [
    "# sheets need to be sorted by date value\n",
    "print('Sorting by date...')\n",
    "final_df = final_df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "print('Saving...')\n",
    "file_name = './data/updated_{}.parquet.gzip'.format(datetime.date(datetime.now()))\n",
    "\n",
    "final_df.astype(str).to_parquet(file_name, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python36864bitanaconda3virtualenv59e2ff4492e04649af7e0fd703909eac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
